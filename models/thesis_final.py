# -*- coding: utf-8 -*-
"""Thesis_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qf8DXBzEKn2HaPcewPWoXTp7nWFKblr6

0.0
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import gzip
import sys, os
import numpy as np
from tqdm.notebook import tqdm
from emoji import UNICODE_EMOJI, demojize
import seaborn as sns
import warnings
import ast
warnings.filterwarnings('ignore')

# %load_ext autoreload
# OPTIONAL: always reload modules so that as you change code in src, it gets loaded
# %autoreload 2
sys.path.insert(0, os.path.abspath('..'))

from src.data import make_dataset, preprocessor

final_df=pd.read_csv(r'twitter.csv')
df = final_df[final_df['language']=='en']
ru_df = final_df[final_df['language']=='ru']
del final_df

emojis_df = make_dataset.get_all_emoji(df, 'en')   
sns.barplot(y="UNICODE", x="count", data=emojis_df.head(20), palette='viridis').set(title='Top 20 Most Used Emojis')

preprocessor.clean_hashtag(df)

counts = make_dataset.count_hashtags(df)
sns.barplot(y=counts.head(20).index, x="hashtag", data=counts.head(20), palette='viridis').set(title='Top 20 Most Used Hashtags')

"""# 1.0"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfTransformer,CountVectorizer,HashingVectorizer

## Reading the data and removing columns that are not important. 
df = pd.read_csv('twitter.csv', sep = ',', encoding = 'latin-1')
df.head(5)

#Remove the data that the text language is not English
df = df.drop(df[df['language']!="en"].index)
#Test on the first 10000 rows
df = df.loc[1:10000]

import re
from bs4 import BeautifulSoup
from html import unescape
#Remove URLS
def remove_urls(x):
    cleaned_string = re.sub(r'(https|http)?:\/\/(\w|\.|\/|\?|\=|\&|\%)*\b', '', str(x), flags=re.MULTILINE)
    return cleaned_string

#Unescape characters
def unescape_stuff(x):
    soup = BeautifulSoup(unescape(x), 'lxml')
    return soup.text

#Remove emojis
def deEmojify(x):
    regrex_pattern = re.compile(pattern = "["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           "]+", flags = re.UNICODE)
    return regrex_pattern.sub(r'', x)

#Replace conservative whitespaces
def unify_whitespaces(x):
    cleaned_string = re.sub(' +', ' ', x)
    return cleaned_string

#Remove unwanted symbols
def remove_symbols(x):
    cleaned_string = re.sub(r"[^a-zA-Z0-9?!.,]+", ' ', x)
    return cleaned_string

df['text'] = df['text'].str.lower()
df['text'] = df['text'].apply(remove_urls)
df['text'] = df['text'].apply(unescape_stuff)
df['text'] = df['text'].apply(deEmojify)
df['text'] = df['text'].apply(remove_symbols)
df['text'] = df['text'].apply(unify_whitespaces)

df['text'].head()

#Transform the words in the text to word frequency matrix
vectorizer = CountVectorizer()
#Calculate tf-idf of each word
transformer = TfidfTransformer()
#Transform texts to word frequency matrix
tfidf = transformer.fit_transform(vectorizer.fit_transform(df["text"]))

#Get all the words
word = vectorizer.get_feature_names()
#Get tf-idf from the matrix
weight = tfidf.toarray()
num = np.array(weight)  
#Number of word lists
print(num.shape[1])

from sklearn.cluster import KMeans
#Cluster to 3 classes
clf = KMeans(n_clusters=3) 
s=clf.fit(weight) 
print(s)         
#Cluster center
print(len(clf.cluster_centers_))

listlabel=[]
i=0
while i<len(clf.labels_):
    listlabel.append([i,clf.labels_[i]])
    i=i+1

frame = pd.DataFrame(listlabel,columns=['index','class'])


list0=[]                                                
data0=frame[(frame[u'class']==0)].iloc[:,0]
for m in data0:
    list0.append(m)

list1=[]
data1=frame[(frame[u'class']==1)].iloc[:,0]
for m in data1:
    list1.append(m)

list2=[]
data2=frame[(frame[u'class']==2)].iloc[:,0]
for m in data2:
    list2.append(m)



# Dimension reduction
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
newData = pca.fit_transform(weight)


#Class 1
x1=[]          
y1=[]        
for j in list0:
    x1.append(newData[j][0])
    y1.append(newData[j][1])

#Class 2
x2 = []
y2 = []
for j in list1:
    x2.append(newData[j][0])
    y2.append(newData[j][1])

#Class 3
x3 = []
y3 = []
for j in list2:
    x3.append(newData[j][0])
    y3.append(newData[j][1])



#Visualization
plt.plot(x1, y1, 'or')
plt.plot(x2, y2, 'og')
plt.plot(x3, y3, 'ob')

plt.show()

print(len(frame[frame['class']==0]))
print(len(frame[frame['class']==1]))
print(len(frame[frame['class']==2]))

print(frame)

frame = frame[['class']]
df

frame.to_csv("labels.csv", sep='\t', encoding='utf-8')

"""#1.1"""

## Reading the data and removing columns that are not important. 
df = pd.read_csv('/content/data/processed/labeled_sample_600.csv', sep = ',', encoding = 'latin-1')
df.head(5)

import re
from bs4 import BeautifulSoup
from html import unescape
#Remove URLS
def remove_urls(x):
    cleaned_string = re.sub(r'(https|http)?:\/\/(\w|\.|\/|\?|\=|\&|\%)*\b', '', str(x), flags=re.MULTILINE)
    return cleaned_string

#Unescape characters
def unescape_stuff(x):
    soup = BeautifulSoup(unescape(x), 'lxml')
    return soup.text

#Remove emojis
def deEmojify(x):
    regrex_pattern = re.compile(pattern = "["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           "]+", flags = re.UNICODE)
    return regrex_pattern.sub(r'', x)

#Replace conservative whitespaces
def unify_whitespaces(x):
    cleaned_string = re.sub(' +', ' ', x)
    return cleaned_string

#Remove unwanted symbols
def remove_symbols(x):
    cleaned_string = re.sub(r"[^a-zA-Z0-9?!.,]+", ' ', x)
    return cleaned_string

df['text'] = df['text'].str.lower()
df['text'] = df['text'].apply(remove_urls)
df['text'] = df['text'].apply(unescape_stuff)
df['text'] = df['text'].apply(deEmojify)
df['text'] = df['text'].apply(remove_symbols)
df['text'] = df['text'].apply(unify_whitespaces)

df['text'].head()

!pip install textblob

from textblob import TextBlob

#Determine the sentiments using polarity of textblob 
accuracy = 0
correct = 0
for i in df.index:
    score = 0
    data = TextBlob(df['text'][i])
    if data.sentiment[0]>0.05:
        score = 2
    elif data.sentiment[0]<-0.05:
        score = 0 
    else:
        score = 1
    print(score,  df['value'][i])
    if  score == df['value'][i]:   
        correct += 1
    else:
        pass

accuracy = correct / len(df)
print(accuracy)

"""#1.2"""



"""#Dataset Preprocessing"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import re
import nltk

df = pd.read_csv(r'twitter.csv')
df.shape
df

df.isnull().sum()

lang_count = df['language'].value_counts()

lang_count.index

lang_count[:10]

hashtag_list = []

def get_hashtag(tweet):
    hashtags = re.findall(r"#(\w+)", tweet)
    hashtag_list.extend(hashtags)

for tweet in df['text']:
    get_hashtag(tweet)

print("total unique hashtags: ", len(hashtag_list))
print(hashtag_list[1:50])

hashtag_set = set(hashtag_list)

def hashtag_freq(hashtag_list):
    a = nltk.FreqDist(hashtag_list)
    hash_df = pd.DataFrame({'Hashtag': list(a.keys()), 
                            'Frequency': list(a.values())
                           })
    
    hash_df = hash_df.nlargest(columns='Frequency', n = 25)
    
    return hash_df

hash_df = hashtag_freq(hashtag_list)

hash_df

from nltk.corpus import stopwords
nltk.download('stopwords')

stopwords = stopwords.words('english')

stemmer = nltk.PorterStemmer()

from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
nltk.download('omw-1.4')

lemmatizer = WordNetLemmatizer()

def preprocess(text):
    text = str(text).lower()
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('\n|\t', '', text)
    text = re.sub('\d', '', text)
    text = re.sub(r'[^\w\s]', ' ', text)
    text = [word for word in text.split() if word not in stopwords]
    text = [stemmer.stem(word) for word in text]
    
    text = " ".join(text)
    
    return text

df['clean_tweet'] = df['text'].apply(preprocess)

avg_tweet_len = df['text'].str.len().mean()
avg_word_count = df['text'].str.split().str.len().mean()

print('Original Tweets: ')
print(int(avg_tweet_len))
print(int(avg_word_count))


avg_clean_tweet_len = df['clean_tweet'].str.len().mean()
avg_clean_word_count = df['clean_tweet'].str.split().str.len().mean()

print('Preprocessed Tweets: ')
print(int(avg_clean_tweet_len))
print(int(avg_clean_word_count))

from textblob import TextBlob

#2- postive 1- Neutral 0-negative
def sentiment_analysis(text):
    analysis = TextBlob(text)
    if (analysis.sentiment.polarity > 0):
        return 2
    elif(analysis.sentiment.polarity<0):
        return 0
    else: 
        return 1

df['sentiment'] = df['clean_tweet'].apply(lambda x : sentiment_analysis(x))

data = df[['clean_tweet', 'sentiment', 'retweetcount', 'favorite_count']]
data1 = df[['clean_tweet', 'sentiment']]
data1

data1.to_csv("pre_process_data.csv", sep='\t', encoding='utf-8')
data

fig = plt.figure(figsize=(10, 6))

plt.bar(hash_df['Hashtag'], hash_df['Frequency'], width=0.4)

plt.xlabel("Top 25 Hashtags")
plt.xticks(rotation=90)
plt.ylabel("Frequency")
plt.show()

text = " ".join(text for text in data['clean_tweet'])

len(text)

from wordcloud import WordCloud

wc = WordCloud(max_font_size=100,  max_words = 50,collocations=False).generate(text)

plt.imshow(wc, interpolation='bilinear')
plt.show()

df_neu = data[data['sentiment'] == 1]
df_neu.head()

neu_text = " ".join(text for text in df_neu['clean_tweet'])

len(neu_text)

wc = WordCloud(max_font_size=100, max_words = 50,collocations=False).generate(pos_text)

plt.imshow(wc, interpolation='bilinear')
plt.show()

df_neg = data[data['sentiment'] == 0]
df_neg.head()

neg_text = " ".join(text for text in df_neg['clean_tweet'])

len(neg_text)

wc = WordCloud(max_font_size=100, max_words = 50,collocations=False).generate(neg_text)

plt.imshow(wc, interpolation='bilinear')
plt.show()

df_pos = data[data['sentiment'] == 2]
df_pos.head()

pos_text = " ".join(text for text in df_pos['clean_tweet'])

len(pos_text)

wc = WordCloud(max_font_size=100, max_words = 50,collocations=False).generate(neg_text)

plt.imshow(wc, interpolation='bilinear')
plt.show()

print("Total Retweet count for Positive Tweets: ", '{:.2f}'.format(df_pos['retweetcount'].sum()))
print("Total Retweet count for Negative Tweets: ", '{:.2f}'.format(df_neg['retweetcount'].sum()))
print("Total Retweet count for Neutral Tweets: ", '{:.2f}'.format(df_neu['retweetcount'].sum()))

print("Total Like count for Positive Tweets: ", '{:.2f}'.format(df_pos['favorite_count'].sum()))
print("Total Like count for Negative Tweets: ", '{:.2f}'.format(df_neg['favorite_count'].sum()))
print("Total Like count for Neutral Tweets: ", '{:.2f}'.format(df_neu['favorite_count'].sum()))

print("Average Retweet count for Positive Tweets: ", '{:.2f}'.format(df_pos['retweetcount'].mean()))
print("Average Retweet count for Negative Tweets: ", '{:.2f}'.format(df_neg['retweetcount'].mean()))
print("Average Retweet count for Neutral Tweets: ", '{:.2f}'.format(df_neu['retweetcount'].mean()))

print("Average Like count for Positive Tweets: ", '{:.2f}'.format(df_pos['favorite_count'].mean()))
print("Average Like count for Negative Tweets: ", '{:.2f}'.format(df_neg['favorite_count'].mean()))
print("Average Like count for Neutral Tweets: ", '{:.2f}'.format(df_neu['favorite_count'].mean()))

x = ['Positive', 'Negative','Neutral']
y = [df_pos['retweetcount'].sum(), df_neg['retweetcount'].sum(),df_neu['retweetcount'].sum()]

plt.bar(x, y, color=['red', 'gray', 'green'])
plt.xlabel('Sentiments')
plt.ylabel('Total Retweet Count')
plt.show()

x = ['Positive', 'Negative', 'Neutral']
y = [df_pos['retweetcount'].mean(), df_neg['retweetcount'].mean(), df_neu['retweetcount'].mean()]

plt.bar(x, y, color=['red', 'gray', 'green'])
plt.xlabel('Sentiments')
plt.ylabel('Average Retweet Count')
plt.show()

x = ['Positive', 'Negative', 'Neutral']
y = [df_pos['favorite_count'].sum(), df_neg['favorite_count'].sum(), df_neu['favorite_count'].sum()]

plt.bar(x, y, color=['red', 'gray', 'green'])
plt.xlabel('Sentiments')
plt.ylabel('Total Like Count')
plt.show()

x = ['Positive', 'Negative', 'Neutral']
y = [df_pos['favorite_count'].mean(), df_neg['favorite_count'].mean(), df_neu['favorite_count'].mean()]

plt.bar(x, y, color=['red', 'gray', 'green'])
plt.xlabel('Sentiments')
plt.ylabel('Average Like Count')
plt.show()

"""#Vectorization"""

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
import gensim

bow_vectorizer = CountVectorizer(max_df = 0.90, min_df=2, max_features = 50000, stop_words='english')

bow = bow_vectorizer.fit_transform(data['clean_tweet'])

bow.shape

tfidf_vectorizer = TfidfVectorizer(max_df = 0.90, min_df=2, max_features = 50000, stop_words='english')

tfidf = tfidf_vectorizer.fit_transform(data['clean_tweet'])

tfidf.shape

"""#Dataset splitting"""

from sklearn.model_selection import train_test_split

X1_train, X1_test, y1_train, y1_test = train_test_split(bow, data['sentiment'], test_size = 0.20, random_state = 0)

print(X1_train.shape)
print(y1_train.shape)
print(X1_test.shape)
print(y1_test.shape)

from sklearn.model_selection import train_test_split

X2_train, X2_test, y2_train, y2_test = train_test_split(tfidf, data['sentiment'], test_size = 0.25, random_state = 0)

print(X2_train.shape)
print(y2_train.shape)
print(X2_test.shape)
print(y2_test.shape)

"""#Model Training
Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report

model = LogisticRegression(max_iter = 1000, random_state = 0)
model.fit(X1_train, y1_train)

y1_pred = model.predict(X1_test)

cm = confusion_matrix(y1_test, y1_pred)
print(cm)

print(classification_report(y1_test, y1_pred))

model.fit(X2_train, y2_train)

y2_pred = model.predict(X2_test)

cm = confusion_matrix(y2_test, y2_pred)
print(cm)

print(classification_report(y2_test, y2_pred))

"""#Naive Bayes"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

model = MultinomialNB()
model.fit(X1_train, y1_train)

y1_pred = model.predict(X1_test)

cm = confusion_matrix(y1_test, y1_pred)

print(cm)

print(classification_report(y1_test, y1_pred))

model.fit(X2_train, y2_train)

y2_pred = model.predict(X2_test)

cm = confusion_matrix(y2_test, y2_pred)
print(cm)

print(classification_report(y2_test, y2_pred))

"""#Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)

model.fit(X1_train, y1_train)

y1_pred = model.predict(X1_test)

cm = confusion_matrix(y1_test, y1_pred)

print(cm)

print(classification_report(y1_test, y1_pred))

model.fit(X2_train, y2_train)

y2_pred = model.predict(X2_test)

cm = confusion_matrix(y2_test, y2_pred)
print(cm)

print(classification_report(y2_test, y2_pred))

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)

model.fit(X1_train, y1_train)

y1_pred = model.predict(X1_test)

cm = confusion_matrix(y1_test, y1_pred)

print(cm)

print(classification_report(y1_test, y1_pred))

model.fit(X2_train, y2_train)

y2_pred = model.predict(X2_test)

cm = confusion_matrix(y2_test, y2_pred)
print(cm)

print(classification_report(y2_test, y2_pred))